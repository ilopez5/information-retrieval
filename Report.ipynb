{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "### Methods Implemented:\n",
    "1. Exact Top K\n",
    "2. Inexact Top K: Champions List\n",
    "3. Inexact Top K: Index Compression\n",
    "4. Inexact Top K: Cluster Pruning\n",
    "\n",
    "### Implementation:\n",
    "Generally speaking, there is one python dictionary where I stored all document vectors across all methods, `self.documents`. Here, the following format was followed:\n",
    "```Python\n",
    "self.documents[document_ID] = [\"Text-#.txt\", [Mthd_1_Vector], [Mthd_2_Vector], [Mthd_3_Vector], [Mthd_4_Vector]]\n",
    "```\n",
    "\n",
    "The reason for this was primarily to keep each method independent of one another, in order to truly compare the inexact variations to the exact.\n",
    "\n",
    "Generally, I created document vectors with elements of tuples containing a term and tf-idf score, such as `(term, tf-idf)`. When computing the dot product of two vectors, I simply iterated through both vectors, and once a term matched, I multiplied their tf-idf scores and added them to the cumulating total. I think with more time, I could implement a way to have it be sorted, as well as populating the gaps, in order to have the indices match for a given term (whether present or not).\n",
    "\n",
    "### Performance\n",
    "To measure performance, I am using **Precision**, **Recall**, and **F-measure**. After testing with 5 equal inquiries, these are my findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
